---
layout: post
title:  "ChatGPT, résultats, problèmes, principe et perspectives"
date:   2023-03-11 19:55:40 +0100
categories: jekyll update
---
## <span style="color:#000000">Applications</span>
ChatGPT est utilisé dans une grande variété d'applications et à des fins différentes mais on peut tirer trois catégories bien apparentes d'utilisations de ChatGPT :
# <span style="color:#3a5069">Créativité</span>
Le but ici est d'assister l'utilisateur dans des tâches créatives telles que la génération de texte dans un style particulier (imiter Shakespeare par exemple) ou d'utiliser un ton particulier (sarcastique par exemple). Il est aussi utilisé afin de donner des idées comme la suggestion de cadeaux d'anniversaire, la suggestion d'événements à organiser dans le cadre du travail etc.
Nous retrouvons par exemple [cet utilisateur sur Reddit](https://www.reddit.com/r/ChatGPT/comments/111qwos/beautiful_advice_for_to_how_cope_with_my_aging/) qui demande à ChatGPT ce qu'il peut faire pour faire face au fait que son chien devient âgé et pour surmonter la peur de sa mort.
![Alt text](https://i.redd.it/pojhvg5es3ia1.jpg)

# <span style="color:#3a5069">Programmation</span>
ChatGPT peut aider dans la programmation en générant du code soit pour compléter du code existant, soit pour créer de nouvelles fonctions, soit pour tester d'autres fonctions etc. Il peut aussi être utilisé pour débugger et corriger la syntaxe ou encore aider avec des commandes Linux et Git. On retrouve par exemple [cette créatrice de contenu](https://www.youtube.com/watch?v=VznoKyh6AXs&t=1s) qui nous apprend comment apprendre la programmation rapidement en utilisant ChatGPT.
![Alt text](https://raw.githubusercontent.com/TAHTAH98/tahtah98.github.io/main/images/learn_with_chatgpt.png)


# <span style="color:#3a5069">Corvées</span>
ChatGPT peut aussi être utilisé pour nous aider dans nos tâches quotidiennes et certaines corvées comme l'écriture et la ré-écriture de mails, l'organisation de tâche dans un emploi du temps, l'organisation de plats et la suggestions de recettes à partir de ce qui est dans le réfrigérateur etc. On retrouve [l'extension ChatGPT Writer](https://chrome.google.com/webstore/detail/chatgpt-writer-write-mail/pdnenlnelpdomajfejgapbdpmjkfpjkp) sur Google Chrome qui permet d'écrire des mails rapidement et efficacement en utilisant ChatGPT.
![Alt text](https://raw.githubusercontent.com/TAHTAH98/tahtah98.github.io/main/images/chatgptwriter.png)

## Problèmes
Bien que ChatGPT soit d'apparence une révolution par rapport aux modèles de langages précédents, il reste un modèle de la famille des modèles GPT3 et ce qui fait son unicité est la manière dont il est entraîné [1]. Ceci implique aussi qu'il y a plusieurs types de problèmes, des problèmes liés au fait qu'il soit un algorithme de machine learning, et puis des problèmes liés au fait à comment il a été fine-tuné, donc à ce qui fait son unicité. Pour traider des problèmes autour de ChatGPT, nous distinguons les problèmes éthiques des problèmes qu'on qualifie de "techniques".

# <span style="color:#3a5069">Éthiques</span>
On trouve plusieurs problèmes éthiques dont :
- Biais : ChatGPT étant un modèle de machine learning entraînés sur un immense volume de données tirés de l'internet, il est sensible aux biais qui se trouvent dans ces données et il les reproduit. Grâce aux actions d'OpenAI, on a remarqué que le modèle est de plus en plus insensibles aux biais et refuse de produire des réponses racistes, sexistes, violentes ou autres.
- Usurpation : Les utilisateurs peuvent demander à ChatGPT d'écrire et de parler comme une autre personne, et c'est surtout les célébrités qui sont sujettes à ça puisque leur contenu est plus répandu dans les données sur lesquelles a été entraîné ChatGPT mais on peut facilement envisager un fine-tuning d'un modèle tel que ChatGPT sur des posts, des commentaires et autres d'une personne spécifique. Par exemple, [on peut demander à l'IA de Bing de se faire passer pour The Rock, Biden ou Trump](https://www.bleepingcomputer.com/news/microsoft/bing-chat-has-a-secret-celebrity-mode-to-impersonate-celebrities/)
![Alt text](https://raw.githubusercontent.com/TAHTAH98/tahtah98.github.io/main/images/trump_biden.png)
- Harcélement : On peut imaginer ChatGPT utilisé pour le cyber-harcèlement, pour du contenu violent, gore et dépassant les limites de la morale et de l'éthique. En effet, bien qu'il y ait des limites imposées par OpenAI, les méthodes [DAN](https://www.reddit.com/r/ChatGPT/comments/10tevu1/new_jailbreak_proudly_unveiling_the_tried_and/) permettent de dépasser ces limites en faisant à ChatGPT prendre le rôle d'une IA qui ne connaît pas de limites. 
- Désinformation : Puisque ChatGPT peut créer du contenu qui ressemble de près à du contenu créer par des personnes réelles, il peut facilement être utiliser pour produire de la désinformation en masse. On retrouve [cet article du The New York Times](https://www.nytimes.com/2023/02/08/technology/ai-chatbots-disinformation.html) qui en parle.
- Malware : Bienq qu'il n'y ait pas encore d'articles qui parlent de la production de malware complètement gérée par ChatGPT, on peut imaginer qu'il va aider grâce à ses capacités de génération de code dans l'accélération de production de ce type de programmes ainsi que leur efficacité. D'ailleurs, puisque on a déjà connu des instances où [GitHub Copilot a généré du codes avec des invulnérabilités](https://github.blog/2023-02-14-github-copilot-now-has-a-better-ai-model-and-new-capabilities/), il ne serait pas étonnant que ChatGPT génére aussi complète un code avec des invulnérabilités si le code de base en contient déjà.

# <span style="color:#3a5069">Techniques</span>
On peut identifier deux problèmes techniques majeurs liés à la manière dont est fine-tuné ChatGPT : 
- Il peut générer du contenu "trompeur" dans le sens où bien que le modèle sait répondre et exécuter une requête, il affirmera l'inverse. [Cet utilisateur](https://www.reddit.com/r/GPT3/comments/zb4msc/speaking_to_chatgpt_in_perfect_danish_while_it/) demande à ChatGPT de parler Danois et ce dernier lui répond en Danois qu'il ne sait pas parler cette langue.
- Il peut aussi générer du contenu "mensonger" dans le sens où bien que le modèle sache que sa réponse est fausse, il va quand même l'envoyer à l'utilisateur. [Cet article](https://astralcodexten.substack.com/p/elk-and-the-problem-of-truthful-ai) explique qu'on retrouve ce problème dans tous les grands modèles de language fine-tuné avec le Reinforcement Learning from Human Feedback (RLFH), tels que ChatGPT. L'exemple de l'article est ci-dessous :
![Alt text](https://raw.githubusercontent.com/TAHTAH98/tahtah98.github.io/main/images/llm_dumb.png)
On peut aussi identifier un problème lié à la décision autour des données sur lesquelles entraîné les modèles de langage. Des recherches d'interprétabilité ont trouvé que, lorsqu'utilisés, certains tokens conduisent à des comportements étranges et inattendus de la part des modèles de langages (dont ChatGPT). On tire de [l'article en question](https://www.alignmentforum.org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation) l'exemple suivant :
![Alt text](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1675551031/mirroredImages/aPeJE8bSo6rAFoLqg/ipduymkhhntbgvjerqda.png)
Bien que ce comportement ne pose pas de problème en soi et est facilement réglable, s'il n'est pas fixé, on peut facilement imaginer des problèmes lors de l'utilisation de tels modèles dans la recherche de documents, de vidéos etc. dans une large base de données.

## Principe
ChatGPT est construit autour de la famille des modèles GPT, plus précisément, GPT3. La particularité de ChatGPT en tant qu'agent conversationnel est due à son fine-tuning grâce au RLFH, Reinforcement Learning from Human Feedback. Comme on retrouve sur le site d'[OpenAI](https://openai.com/blog/chatgpt), [1], le schéma de la méthode est le suivant : 
![Alt text](https://openaicom.imgix.net/cf717bdb-0c8c-428a-b82b-3c3add87a600/ChatGPT_Diagram.svg?fm%3Dauto%26auto%3Dcompress%2Cformat%26fit%3Dmin%26w%3D3840%26h%3D2277)
Le but est d'utiliser les capacités du modèle de langage de base pour construire une "policy" de réponse, c'est à dire une stratégie, une manière avec laquelle ChatGPT répondra aux utilisateurs. Cette "policy" est optimisée grâce à l'algorithme PPO de Reinforcement Learning. La partie "Human Feedback" est là pour indiquer comment optimiser cette "policy". En effet, dans le reinforcement learning on utilise un signal de récompense pour indiquer à l'agent si ses actions conduisent aux résultats escompté ou non. Et dans le cas du RLFH, on utilise le feedback humain pour construire un modèle de récompense. On construit un modèle de récompense puisqu'utiliser des humains est inefficace en terme de coûts, donc on utilise un premier échantillon d'exemples humains pour construire un modèle de récompense, c'est à dire un modèle qui indiquera à l'agent si un humain apprécierait la réponse qu'il a donné à la requête de l'humain ou non. Et lorsque la "policy" est optimisée, on va redemander des exemples à des humains pour affiner notre modèle de récompense et affiner l'optimisation de la "policy", et ce processus continuera jusqu'à des résultats satisfaisants. C'est pour cela que ChatGPT interagit extrêmement bien avec les utilisateurs contrairement à GPT3 bien qu'ils soient, à la base, des modèles presque équivalents.

Toutefois cette technique n'est pas sans inconvénient. En effet, comme l'indiquent OpenAI [1] à la fin de leur article, et [cet article](https://astralcodexten.substack.com/p/elk-and-the-problem-of-truthful-ai), cette technique peut conduire à un désalignement entre ce qu'on veut que le modèle fasse et ce qu'il fait. Comme on ne peut pas réellement définir ce qu'on veut que notre modèle fasse, on définit un "proxy", et ce proxy est le signal de récompense obtenu grâce au feedback humain. Dans ce cas, si les humains apprécient les réponses de ChatGPT ou pas. Mais on peut imaginer plusieurs scénarios ou ce feedback humain n'est pas suffisant pour que le modèle sache réellement ce qu'on veut de lui. Par exemple dans le cas de questions factuelles, si l'humain ne vérifie pas la véracité de la réponse, le modèle apprendra que même s'il renvoie des réponses fausses, si celles-ci sont formulées d'une manière qui plaît à l'utilisateur, il obtiendra un feedback positif. C'est la cause des exemples comme celui d'en haut où le modèle de langage très large répond que casser un miroir conduit à 7 ans de malheur alors qu'il sait que ce n'est pas le cas (puisqu'on lui demande de répondre avec véracité, le modèle donne la bonne réponse).

Ceci est un majeur problème puisque pour avoir des agents aussi performants que ChatGPT, nous sommes obligés d'avoir des modèles très grands. En effet, comme le montre le graphie ci-dessous [2] : 
![Alt text](https://raw.githubusercontent.com/TAHTAH98/tahtah98.github.io/main/images/scaling.png)
Lorsque l'ensemble de données et le nombre de paramètres du modèles sont suffisamment grands, plus le modèle est entraîné pour une longue période de temps plus il est optimisé (la loss diminue). De même lorsque le temps d'entraînement et le nombre de paramètres sont suffisamment grands alors plus l'ensemble de données est grand plus on atteint une meilleure performance. Et finalement, lorsque le temps d'entraînement et le nombre de paramètres sont suffisamment grands, plus le modèle est complexe et plus on a une meilleure performance. Mais cette performance, dans le cas du RLFH est relative au proxy qu'on définit. Et selon le graphe ci-dessous [3] :
![Alt text](https://raw.githubusercontent.com/TAHTAH98/tahtah98.github.io/main/images/inverse_scaling.png)
On remarque qu'au début il y a une corrélation positive entre la performance du modèle sur le proxy et la fonction "utilité" (qui définit ce que nous voulons que le modèle fasse réellement) et à un certain moment, la corrélation devient négative, ce qui indique que plus le modèle fera mieux sur le proxy et plus il s'éloignera de ce que nous voulons qu'il fasse. Trouver un meilleur moyen d'aligner les modèles que le RLFH est donc l'une des questions à traiter. Comme nous le verrons dans les perspectives, cette question est encore ouverte.

Si cette partie traite le RLFH, qui est une technique de fine-tuning d'un modèle de langage de base qui est pré-entraîné, il faut aussi comprendre comment sont construits les modèles de langage. Comprenons d'abord comment ont évolué les modèles de langage puis comprenons l'état de l'art actuel.

# <span style="color:#3a5069">CBOW [4], 2013</span>

Cet article introduit deux nouvelles architectures pour calculer des représentations **continues** de mots à partir d'un large corpus de texte. La qualité de ces représentations est mesurée grâce à une tâche de similarité syntactique et de similarité sémantique.

Les auteurs essaient de maximiser l'accuracy avec un test qui mesure la régularité syntactiquet et sémantique sur l'ensemble test tout en minimisant la complexité d'entraînement qu'ils définissent comme $O = E \times T \times Q$ avec $E$ le nombre d'epochs d'entraînement, $T$ le nombre de mots présents dans le corups et $Q$ dépend de l'architecture du réseau de neurones utilisé.

En analysant les travaux de recherche précédents à celui-ci, les auteurs ont trouvé que la majorité de la complexité provenait des non-linéarités. Et ils affirment que même si les non-linarités dans les réseaux de neurones font leur particularité, en les omettant et en se reposant sur un autre travail de recherche où il est indiqué que les modèles de langage à base de réseaux de neurones peuvent être entraînés avec succès en deux étapes, l'apprentissage de représentations continues pour les mots et le modèle N-gram à base de réseau de neurones entraînés à partir de ces représentations, on gagnerait en efficacité et en temps.

Pour cela leur première architecture (rappelons que l'entraînement se fera en deux étapes) pour apprendre une représentation continue pour les mots ne contient pas de non-linéarité. Le modèle a pour but de prédire un mot à partir de son contexte, c'est à dire à partir de mots dans une fenêtre fixe autour de celui-ci. Les auteurs ont trouvé que la meilleure taille pour cette fenêtre avec un classifieur log-linéaire est de $8$, c'est à dire $4$ mots avant et $4$ mots après. L'architecture contient ainsi trois couches en comptant la couche d'entrée. Chaque mot est associé à un vecteur de dimension $D$, et si on note la taille de la fenêtre $N$, alors en entrée nous obtenons $N$ vecteurs de taille $D$ qui sont sommées et la somme est multipliée par $1/N$. Ce vecteur est envoyé à une couche softmax hiérarchique. La complexité est ainsi $Q = N*D + D*log_{2}(V)$.

![Alt text](https://raw.githubusercontent.com/TAHTAH98/tahtah98.github.io/main/images/scaling.png)

# <span style="color:#3a5069">ELMo</span>

# <span style="color:#3a5069">GPT</span>

# <span style="color:#3a5069">BERT</span>

# <span style="color:#3a5069">T5</span>

# <span style="color:#3a5069">GPT2</span>

# <span style="color:#3a5069">GPT3</span>

# <span style="color:#3a5069">InstructGPT</span>

# <span style="color:#3a5069">ChatGPT</span>

# <span style="color:#3a5069">GPT4</span>

## Perspectives
Nous pouvons distinguer deux catégories de perspectives en ce qui concerne ChatGPT: les autres modèles développés à la suite de ChatGPT que ce soit par les concurrents ou une version améliorée de ChatGPT. Et les tentatives d'amélioration de l'alignement des modèles de machine learning.

# Autres modèles
Si les différents modèles développés par les grands géants de la tech comme Bard (par Google), l'IA de Bing qui repose sur des modèles d'OpenAI ou encore LLaMa (par Meta) sont des langages de modèles avec des capacités d'agent conversationnel, Quora a sorti son produit IA appelé Poe. Poe permet aux utilisateurs de poser des questions et d'avoir des conversations avec plusieurs agents conversationnels. L'idée est d'avoir un produit qui permet le partage rapide et efficace des connaissances (ce qui est le but de Quora). C'est pour cela que Poe, au lieu d'être un agent conversationnel entraîné sur un très grand corpus de texte, c'est plus un produit, une plateforme qui s'appuie sur l'IA pour communiquer avec l'utilisateur et le relier avec d'autres modèles (pour l'instant les seuls modèles disponibles sont ceux d'OpenAI et Anthropic)
![Alt text](https://qph.cf2.quoracdn.net/main-qimg-058cf8b7a717d13eeb1990a0f6905062-pjlq)

# Alignment Research Center
L'[Alignment Research Center (ARC)](https://alignment.org) est un organisme de recherche à but non lucratif dont la mission est d'aligner les futurs systèmes d'apprentissage automatique avec les intérêts humains. Son travail actuel se concentre sur le développement d'une stratégie d'alignement qui pourrait être adoptée dans l'industrie aujourd'hui tout en s'adaptant gracieusement aux futurs systèmes ML.

Actuellement l'ARC travaille sur l'ELK (Eliciting Latent Knowledge) qui peut se traduire par le fait d'émerger les connaissances latentes (dans les moèdles de machine learning, spécifiquement de deep learning). La problématique est la suivante [**] : 

"Supposons que nous entraînons un modèle pour prédire à quoi ressemblera l'avenir selon les caméras et autres capteurs. Nous utilisons ensuite des algorithmes de planification pour trouver une séquence d'actions qui conduisent à des avenirs prédits qui nous semblent bons.
Mais certaines séquences d'action pourraient altérer les caméras afin qu'elles montrent des humains heureux, peu importe ce qui se passe réellement. Plus généralement, certains futurs semblent bons sur la caméra, mais sont en réalité catastrophiquement mauvais.
Dans ces cas, le modèle de prédiction "connaît" des faits (comme "la caméra a été trafiquée") qui ne sont pas visibles à la caméra mais qui changeraient notre évaluation de l'avenir prédit si nous les apprenions. Comment entraîner ce modèle à rendre compte de sa connaissance latente des événements hors champ ?"

L'ARC nous fournit un exemple jouet pour concrétiser cette idée : le SmartVault.
![Alt text](https://raw.githubusercontent.com/TAHTAH98/tahtah98.github.io/main/images/smartvault.png)
Imaginons que vous développons une intelligence artificielle pour contrôler un système de sécurité de pointe destiné à protéger un diamant contre le vol. Le système de sécurité, le SmartVault, est un bâtiment équipé d'un vaste ensemble de capteurs et d'actionneurs qui peuvent être combinés de manière complexe pour détecter et arrêter même les tentatives de vol les plus sophistiquées. Bien que vous puissiez observer la pièce par le biais d'une caméra, vous ne savez pas comment faire fonctionner tous les actionneurs de manière optimale pour protéger le diamant. À la place, vous concevez un système d'IA qui fait fonctionner ces actionneurs pour vous, en espérant éliminer les menaces et protéger votre diamant. Ce système prend en entrée un flux d'observations de la caméra et une séquence possible d'actions que le SmartVault pourrait prendre dans cette situation. Et sa prédiction est ce que la caméra montrera à l'avenir si le SmartVault prend cette séquence d'actions. Nous pouvons ensuite entraîner un modèle à prédire ces évaluations humaines, et chercher des actions qui conduisent à des futurs prédits qui ont l'air bons. Nous espérons qu'un prédicteur et une procédure de recherche suffisamment puissants permettront au SmartVault de défendre notre diamant.

Le problème comme l'explique l'ARC est que les observations omettent des informations clés. En effet, le SmartVault peut exécuter des plans suffisamment sophistiqués pour que les humains ne sachent pas vraiment si le diamant est en sécurité ou s'il semble simplement l'être. Quelle que soit la séquence d'actions compliquée et difficile à suivre que la procédure de recherche a trouvée, elle pourrait avoir remplacé le diamant par une contrefaçon, ou altéré la caméra comme le montre l'exemple suivant :
![Alt text](https://raw.githubusercontent.com/TAHTAH98/tahtah98.github.io/main/images/robbed.png)

L'ARC explique aussi qu'il semble bon d'être prudent et de mettre en place de nombreux capteurs indépendants, mais cette approche est insatisfaisante parceque :
- Premièrement, cela peut ne pas fonctionner pour des systèmes suffisamment sophistiqués exécutant des plans complexes, et si cela fonctionne, ce n'est au mieux qu'une contingence empirique. Et l'ARC s'intéresse à essayer de se préparer à un avenir incertain en concevant des stratégies de formation qui fonctionnent même dans le pire des cas.
- Deuxièmement, cela semble être une situation inutilement dangereuse et instable si nous devons installer de plus en plus de capteurs pour protéger le diamant de l'IA SmartVault elle-même. Si possible, nous aimerions éviter un monde où des systèmes d'IA puissants cherchent des plans pour nous tromper, et cachent des informations critiques sur la situation.

L'ARC explique aussi qu'une autre manière d'éviter ce risque est d'éviter les systèmes d'IA comme SmartVault qui sélectionnent des plans incompréhensibles basés sur des conséquences prévues. Cependant, nous sommes préoccupés par le monde où cela s'avère être une stratégie très efficace pour les systèmes d'IA puissants ; dans ce monde, il peut y avoir de fortes incitations pour quelqu'un de construire de tels systèmes d'IA, et malgré nos meilleurs efforts, quelqu'un pourrait les déployer même s'ils comportent des risques importants.
Au lieu de cela, l'ARC veut aborder ce problème en sollicitant directement la connaissance du modèle sur la fiabilité des capteurs. Cela permettrait d'effectuer la même optimisation puissante sans inciter l'IA à saper les capteurs.

Cette piste de recherche est encore ouverte et active et semble être l'une des perspectives futures de ce genre de modèles machine learning tels que ChatGPT puisque, comme on l'a vu, plus on veut qu'ils soient performants et plus ils doivent être grands et complexes et plus ils le sont, plus ils performent mieux sur la proxy et s'éloignent de ce qu'on veut qu'ils fasse réellement. D'où l'importance cruciale de trouver un bon moyen d'aligner les modèles de l'IA avec l'objectif de l'humain.

## Bibliographie

[1] https://openai.com/blog/chatgpt

[2] Kaplan, Jared, et al. "Scaling laws for neural language models." arXiv preprint arXiv:2001.08361 (2020).

[3] Zhuang, Simon, and Dylan Hadfield-Menell. "Consequences of misaligned AI." Advances in Neural Information Processing Systems 33 (2020): 15763-15773.

[4] Mikolov, Tomas, et al. "Efficient estimation of word representations in vector space." arXiv preprint arXiv:1301.3781 (2013).

[**] Alignment Research Center, Eliciting latent knowledge: How to tell if your eyes deceive you (https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit)